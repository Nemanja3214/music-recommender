{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:36:23.500256Z",
     "start_time": "2025-11-22T00:36:20.571075Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install ipywidgets",
   "id": "1f17a35b397bd328",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\r\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in ./venv/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.11/site-packages (from ipywidgets) (9.7.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\r\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\r\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\r\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: decorator>=4.3.2 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\r\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.18.1 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\r\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\r\n",
      "Requirement already satisfied: pygments>=2.11.0 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\r\n",
      "Requirement already satisfied: stack_data>=0.6.0 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: typing_extensions>=4.6 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.11/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\r\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\r\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\r\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m914.9/914.9 kB\u001B[0m \u001B[31m8.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\r\n",
      "Successfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:03:24.975599Z",
     "start_time": "2025-11-22T00:03:10.142935Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install torch_geometric",
   "id": "69653c71d438ba06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\r\n",
      "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\r\n",
      "Collecting aiohttp (from torch_geometric)\r\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\r\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch_geometric) (2025.10.0)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch_geometric) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from torch_geometric) (2.3.5)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./venv/lib/python3.11/site-packages (from torch_geometric) (7.1.3)\r\n",
      "Collecting pyparsing (from torch_geometric)\r\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\r\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from torch_geometric) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from torch_geometric) (4.67.1)\r\n",
      "Collecting xxhash (from torch_geometric)\r\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\r\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->torch_geometric)\r\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->torch_geometric)\r\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->torch_geometric) (25.4.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\r\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\r\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch_geometric)\r\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\r\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch_geometric)\r\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch_geometric) (3.0.3)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->torch_geometric) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->torch_geometric) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->torch_geometric) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->torch_geometric) (2025.11.12)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2 in ./venv/lib/python3.11/site-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\r\n",
      "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\r\n",
      "Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\r\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\r\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\r\n",
      "Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\r\n",
      "Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\r\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\r\n",
      "Installing collected packages: xxhash, pyparsing, propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch_geometric\r\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 frozenlist-1.8.0 multidict-6.7.0 propcache-0.4.1 pyparsing-3.2.5 torch_geometric-2.7.0 xxhash-3.6.0 yarl-1.22.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:21:18.498555Z",
     "start_time": "2025-11-22T01:21:13.457047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "# PyG hetero stuff\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "\n",
    "\n",
    "\n",
    "    # Constants\n",
    "AUDIO_FEATURE_KEYS = [\n",
    "    'danceability','energy','valence','tempo','loudness',\n",
    "    'speechiness','instrumentalness','liveness','acousticness'\n",
    "]\n",
    "\n",
    "SEED = 42"
   ],
   "id": "acfe4c5a3f93420f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:21:18.531329Z",
     "start_time": "2025-11-22T01:21:18.521949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2: MPD iterator & small sample fallback\n",
    "\n",
    "def iter_mpd_playlists(mpd_dir: str, n=None):\n",
    "    \"\"\"Yield playlists from each .json file in mpd_dir (sorted).\"\"\"\n",
    "    files = [os.path.join(mpd_dir, f) for f in os.listdir(mpd_dir) if f.endswith('.json')]\n",
    "    if n is None:\n",
    "        files = sorted(files)\n",
    "    else:\n",
    "        files = sorted(files)[:n]\n",
    "\n",
    "    print(f\"There is {len(files)} files\")\n",
    "    for fp in files:\n",
    "        with open(fp, 'r', encoding='utf-8') as fh:\n",
    "            try:\n",
    "                data = json.load(fh)\n",
    "            except Exception as e:\n",
    "                print(f'Warning: failed to load {fp}: {e}')\n",
    "                continue\n",
    "            for pl in data.get('playlists', []):\n",
    "                yield pl\n",
    "\n"
   ],
   "id": "39aa73c2ff106efd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:21:18.576843Z",
     "start_time": "2025-11-22T01:21:18.574540Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c4556eb5eb023bcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:21:18.681976Z",
     "start_time": "2025-11-22T01:21:18.624603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import http.client\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"api.reccobeats.com\")\n",
    "payload = ''\n",
    "headers = {\n",
    "  'Accept': 'application/json'\n",
    "}\n"
   ],
   "id": "63745e03cfa03db9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:23:25.314665Z",
     "start_time": "2025-11-22T01:23:25.278695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augment_tracks(playlists: List[Dict], sp=None, cache_path: str = None, batch=50):\n",
    "    \"\"\"Return dict track_uri -> feature dict for AUDIO_FEATURE_KEYS. Use cache if available.\"\"\"\n",
    "    unique = {}\n",
    "    for pl in playlists:\n",
    "        for t in pl['tracks']:\n",
    "            unique[t['track_uri']] = t\n",
    "    tids = list(unique.keys())\n",
    "    features = {}\n",
    "    cache = {}\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as fh:\n",
    "                cache = json.load(fh)\n",
    "        except Exception:\n",
    "            cache = {}\n",
    "\n",
    "    def af_to_vec(af):\n",
    "        if af is None:\n",
    "            return {k: 0.0 for k in AUDIO_FEATURE_KEYS}\n",
    "        return {k: float(af.get(k, 0.0)) for k in AUDIO_FEATURE_KEYS}\n",
    "\n",
    "\n",
    "    if sp is None:\n",
    "        for tid in tqdm(tids, desc='augment (synth)'):\n",
    "            if tid in cache:\n",
    "                features[tid] = cache[tid]; continue\n",
    "            vec = {k: float(np.random.rand()) for k in AUDIO_FEATURE_KEYS}\n",
    "            vec['tempo'] = 60.0 + np.random.rand() * 120.0\n",
    "            vec['loudness'] = -60.0 + np.random.rand() * 60.0\n",
    "            features[tid] = vec\n",
    "            cache[tid] = vec\n",
    "    else:\n",
    "        tid2sp = {tid: tid.split(':')[-1] for tid in tids}\n",
    "        batch_ids, batch_tids = [], []\n",
    "        for tid, spid in tqdm(tid2sp.items(), desc='augment (spotify)'):\n",
    "            if tid in cache:\n",
    "                features[tid] = cache[tid]; continue\n",
    "            batch_ids.append(spid); batch_tids.append(tid)\n",
    "            if len(batch_ids) >= batch:\n",
    "                try:\n",
    "                    conn.request(\"GET\", f\"/v1/track/{tid}/audio-features\", payload, headers)\n",
    "                    res = conn.getresponse()\n",
    "                    data = res.read()\n",
    "                    afs = data.decode(\"utf-8\")\n",
    "                    print(afs)\n",
    "                except Exception as e:\n",
    "                    print('Spotify API error:', e); afs = [None]*len(batch_ids)\n",
    "                for bt, af in zip(batch_tids, afs):\n",
    "                    vec = af_to_vec(af)\n",
    "                    features[bt] = vec; cache[bt] = vec\n",
    "                batch_ids, batch_tids = [], []\n",
    "        if batch_ids:\n",
    "            try:\n",
    "\n",
    "                afs = sp.audio_features(batch_ids)\n",
    "            except Exception as e:\n",
    "                print('Spotify API error:', e); afs = [None]*len(batch_ids)\n",
    "            for bt, af in zip(batch_tids, afs):\n",
    "                vec = af_to_vec(af)\n",
    "                features[bt] = vec; cache[bt] = vec\n",
    "\n",
    "    if cache_path:\n",
    "        try:\n",
    "            with open(cache_path, 'w', encoding='utf-8') as fh:\n",
    "                json.dump(cache, fh)\n",
    "        except Exception as e:\n",
    "            print('Warning: failed to write cache:', e)\n",
    "    return features\n"
   ],
   "id": "8d94a5dfb7573e83",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:23:27.381061Z",
     "start_time": "2025-11-22T01:23:27.314040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 4: build full heterogeneous KG (NetworkX + PyG HeteroData)\n",
    "\n",
    "def build_full_kg(playlists: List[Dict], track_features: Dict[str, Dict]=None):\n",
    "    nxg = nx.MultiDiGraph()\n",
    "    node_ids = {nt: {} for nt in ['playlist','track','artist','album']}\n",
    "    counters = {nt: 0 for nt in node_ids}\n",
    "\n",
    "    # Build NX graph and maps\n",
    "    for pl in playlists:\n",
    "        pid_raw = pl.get('pid') or pl.get('uri') or f\"pl:{time.time()}:{random.randint(0,1e6)}\"\n",
    "        pid = f'playlist:{pid_raw}'\n",
    "        if pid not in node_ids['playlist']:\n",
    "            node_ids['playlist'][pid] = counters['playlist']; counters['playlist'] += 1\n",
    "            nxg.add_node(pid, type='playlist')\n",
    "        for t in pl['tracks']:\n",
    "            tid = t.get('track_uri')\n",
    "            print(tid)\n",
    "            art = t.get('artist_uri', 'artist:unknown')\n",
    "            alb = t.get('album_uri', 'album:unknown')\n",
    "            if tid not in node_ids['track']:\n",
    "                node_ids['track'][tid] = counters['track']; counters['track'] += 1\n",
    "                nxg.add_node(tid, type='track')\n",
    "            if art not in node_ids['artist']:\n",
    "                node_ids['artist'][art] = counters['artist']; counters['artist'] += 1\n",
    "                nxg.add_node(art, type='artist')\n",
    "            if alb not in node_ids['album']:\n",
    "                node_ids['album'][alb] = counters['album']; counters['album'] += 1\n",
    "                nxg.add_node(alb, type='album')\n",
    "            nxg.add_edge(pid, tid, relation='contains')\n",
    "            nxg.add_edge(tid, art, relation='by')\n",
    "            nxg.add_edge(tid, alb, relation='on')\n",
    "\n",
    "    # # Build HeteroData\n",
    "    data = HeteroData()\n",
    "    for ntype in ['playlist','track','artist','album']:\n",
    "        ncount = counters[ntype]\n",
    "        if ncount == 0:\n",
    "            data[ntype].x = torch.zeros((0, len(AUDIO_FEATURE_KEYS)))\n",
    "            continue\n",
    "        if ntype == 'track':\n",
    "            track_list = [None] * ncount\n",
    "            feat_list = [None] * ncount\n",
    "            for tid, idx in node_ids['track'].items():\n",
    "                track_list[idx] = tid\n",
    "                tf = track_features.get(tid, {k: 0.0 for k in AUDIO_FEATURE_KEYS})\n",
    "                feat_list[idx] = np.array([tf.get(k, 0.0) for k in AUDIO_FEATURE_KEYS], dtype=float)\n",
    "            feats_np = np.vstack(feat_list)\n",
    "            feats_np = StandardScaler().fit_transform(feats_np)\n",
    "            data['track'].x = torch.tensor(feats_np, dtype=torch.float)\n",
    "            data['track'].tid_list = track_list\n",
    "        else:\n",
    "            inverse = [None] * counters[ntype]\n",
    "            for k, v in node_ids[ntype].items():\n",
    "                inverse[v] = k\n",
    "            feat_list = []\n",
    "            for node in inverse:\n",
    "                deg = nxg.degree(node)\n",
    "                feat_list.append([float(deg)])\n",
    "            feats_np = np.vstack(feat_list) if len(feat_list)>0 else np.zeros((0,1))\n",
    "            if feats_np.shape[0] > 0:\n",
    "                feats_np = StandardScaler().fit_transform(feats_np)\n",
    "                data[ntype].x = torch.tensor(feats_np, dtype=torch.float)\n",
    "            else:\n",
    "                data[ntype].x = torch.zeros((0,1), dtype=torch.float)\n",
    "            data[ntype].id_list = inverse\n",
    "    #\n",
    "    # # edges: playlist->track, track->artist, track->album with reverse relations\n",
    "    # def edges_from_nx(source_type, target_type, relation):\n",
    "    #     src_idxs = []\n",
    "    #     dst_idxs = []\n",
    "    #     for u, v, d in nxg.edges(data=True):\n",
    "    #         if d.get('relation') != relation:\n",
    "    #             continue\n",
    "    #         if nxg.nodes[u].get('type') != source_type or nxg.nodes[v].get('type') != target_type:\n",
    "    #             continue\n",
    "    #         src_idxs.append(node_ids[source_type][u])\n",
    "    #         dst_idxs.append(node_ids[target_type][v])\n",
    "    #     if len(src_idxs) == 0:\n",
    "    #         return torch.empty((2,0), dtype=torch.long)\n",
    "    #     return torch.tensor([src_idxs, dst_idxs], dtype=torch.long)\n",
    "    #\n",
    "    # data['playlist', 'contains', 'track'].edge_index = edges_from_nx('playlist','track','contains')\n",
    "    # if data['playlist', 'contains', 'track'].edge_index.numel() > 0:\n",
    "    #     data['track', 'rev_contains', 'playlist'].edge_index = data['playlist', 'contains', 'track'].edge_index.flip(0)\n",
    "    # else:\n",
    "    #     data['track', 'rev_contains', 'playlist'].edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "    #\n",
    "    # data['track', 'by', 'artist'].edge_index = edges_from_nx('track','artist','by')\n",
    "    # if data['track','by','artist'].edge_index.numel() > 0:\n",
    "    #     data['artist', 'rev_by', 'track'].edge_index = data['track','by','artist'].edge_index.flip(0)\n",
    "    # else:\n",
    "    #     data['artist', 'rev_by', 'track'].edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "    #\n",
    "    # data['track', 'on', 'album'].edge_index = edges_from_nx('track','album','on')\n",
    "    # if data['track','on','album'].edge_index.numel() > 0:\n",
    "    #     data['album', 'rev_on', 'track'].edge_index = data['track','on','album'].edge_index.flip(0)\n",
    "    # else:\n",
    "    #     data['album', 'rev_on', 'track'].edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "    #\n",
    "    return nxg, node_ids\n"
   ],
   "id": "e29a34f3753b616b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:23:29.492710Z",
     "start_time": "2025-11-22T01:23:28.003144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mpd_dir = \"./archive/data\"  # e.g. \"/home/user/datasets/spotify_mpd\"\n",
    "use_spotify = True\n",
    "spotify_client_id = None\n",
    "spotify_client_secret = None\n",
    "cache_path = 'track_features_cache.json'\n",
    "sample_rate = 0.05\n",
    "limit = 2000\n",
    "device = 'cpu'\n",
    "epochs = 8\n",
    "\n",
    "playlists = []\n",
    "for pl in iter_mpd_playlists(mpd_dir, 10):\n",
    "    playlists.append(pl)\n",
    "    if limit and len(playlists) >= limit:\n",
    "        break\n",
    "\n",
    "print(f'Loaded {len(playlists)} playlists')"
   ],
   "id": "811fe7c0e97c6b09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 10 files\n",
      "Loaded 2000 playlists\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:23:33.123469Z",
     "start_time": "2025-11-22T01:23:29.533035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "if not os.path.exists(cache_path):\n",
    "    with open(cache_path, 'w') as file:\n",
    "        file.write(\"This is a new file created because it didn't exist.\\n\")\n",
    "    print(f\"File '{cache_path}' created successfully.\")\n",
    "else:\n",
    "    print(f\"File '{cache_path}' already exists. No new file was created.\")\n",
    "track_features = augment_tracks(playlists, cache_path=cache_path)\n",
    "\n",
    "try:\n",
    "    os.remove(cache_path)\n",
    "    print(f\"File '{cache_path}' deleted successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{cache_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "# nxg, node_id_maps = build_full_kg(playlists)\n",
    "# print('HeteroData node types:', hetero_data.node_types, 'edge types:', hetero_data.edge_types)\n",
    "# print(nxg)\n"
   ],
   "id": "b7bafea8ee00c5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'track_features_cache.json' created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "augment (synth): 100%|██████████| 57884/57884 [00:00<00:00, 73359.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'track_features_cache.json' deleted successfully.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:22:04.885756Z",
     "start_time": "2025-11-22T01:22:04.184383Z"
    }
   },
   "cell_type": "code",
   "source": "print(track_features)",
   "id": "ec4d0cc673aec0d2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3f9503769002d201",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
